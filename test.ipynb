{"cells":[{"cell_type":"code","source":["!curl https://ollama.ai/install.sh | sh"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5ZW4ON-Aw7w","executionInfo":{"status":"ok","timestamp":1717567719610,"user_tz":-480,"elapsed":5482,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"outputId":"5cc4aace-21fd-470e-970e-4a4d6a5452a0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0>>> Downloading ollama...\n","100 10941    0 10941    0     0  41621      0 --:--:-- --:--:-- --:--:-- 41759\n","############################################################################################# 100.0%\n",">>> Installing ollama to /usr/local/bin...\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n"]}]},{"cell_type":"code","source":["! ollama pull cwchang/llama3-taide-lx-8b-chat-alpha1-32k"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e_fCUyHUAYv_","executionInfo":{"status":"ok","timestamp":1717567733685,"user_tz":-480,"elapsed":392,"user":{"displayName":"CAP AI","userId":"15143934473145264642"}},"outputId":"4a3e185f-b6f0-4d5e-b81c-dd49c948719a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: could not connect to ollama app, is it running?\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CssDn9HPAYU0"},"outputs":[],"source":["! pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all bs4 jq googletrans==4.0.0-rc1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05g3Zmv3AYU2"},"outputs":[],"source":["### LLM\n","\n","local_llm = \"cwchang/llama3-taide-lx-8b-chat-alpha1-32k\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuPlxLjqAYU3"},"outputs":[],"source":["### Index\n","import json\n","from langchain.schema import Document\n","from langchain_community.document_loaders.json_loader import JSONLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_community.embeddings import GPT4AllEmbeddings\n","\n","\n","def load_json_data(filepath):\n","    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n","        json_data = json.load(file)\n","    documents = [Document(page_content=item[\"text\"]) for item in json_data]\n","    return documents\n","\n","data = load_json_data(\"./data.json\")\n","\n","#loader = JSONLoader(\"./data2.json\", jq_schema='.[].text')\n","#data = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=250, chunk_overlap=0\n",")\n","\n","doc_splits = text_splitter.split_documents(data)\n","\n","vectorstore = Chroma.from_documents(\n","    documents=doc_splits,\n","    collection_name=\"rag-chroma\",\n","    embedding=GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\"),\n",")\n","retriever = vectorstore.as_retriever()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABEe8mBwAYU3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hV__BcETAYU3","outputId":"0254a94a-86bc-4386-895f-d431f2f330d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["I want to take a very cold class, and recommend me some courses?\n"]}],"source":["import googletrans\n","question = \"我想要修很涼的課，推薦我一些課程的名字?\"\n","translator = googletrans.Translator()\n","result = translator.translate(question, dest='en')\n","print(result.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaCpr5HtAYU4","outputId":"ecf18568-e5d0-4df1-e4fa-fbadd2772051"},"outputs":[{"name":"stdout","output_type":"stream","text":["根據提供的上下文，我建議以下類：\n","\n","*當代世界：環境危機和生態可持續性（Wang Congshu教授） - 此類似乎是一個輕鬆而簡單的課程，重點是環境問題。\n","\n","請注意，這些建議僅基於提供的上下文，可能不會反映課程的實際質量或困難。\n"]}],"source":["### Generate\n","import googletrans\n","from langchain_community.chat_models import ChatOllama\n","from langchain.prompts import PromptTemplate\n","from langchain import hub\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Prompt\n","prompt = PromptTemplate(\n","    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n","    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n","    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n","    Question: {question}\n","    Context: {context}\n","    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n","    input_variables=[\"question\", \"document\"],\n",")\n","\n","llm = ChatOllama(model=local_llm, temperature=0)\n","\n","\n","# Post-processing\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","# Chain\n","rag_chain = prompt | llm | StrOutputParser()\n","\n","### 測試多個問題\n","questions = [\n","    \"我想要修很涼的課，推薦我一些課程的名字?\"\n","]\n","\n","results = []\n","\n","for question in questions:\n","    # 直接使用繁體中文問題\n","    print(f\"Question: {question}\")\n","\n","    # 檢索相關文檔\n","    docs = retriever.invoke(question)\n","    formatted_docs = format_docs(docs)\n","\n","    # 生成回答\n","    generation = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n","\n","    # 打印生成的回答\n","    print(f\"Generated Answer: {generation}\\n\")\n","\n","    # 保存結果\n","    results.append({\"question\": question, \"answer\": generation})\n","\n","# 打印所有結果\n","for result in results:\n","    print(f\"Question: {result['question']}\")\n","    print(f\"Answer: {result['answer']}\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}